{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2724750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
    "\n",
    "# STEP 1: Load your data and filter\n",
    "import pandas as pd\n",
    "\n",
    "# Ignore dtype warnings by loading only relevant columns\n",
    "df = pd.read_csv('/home/ri_rishna_urandugp/.vscode/Transformers/NCERT_dataset.csv', \n",
    "                 usecols=['Explanation', 'Question', 'Answer', 'subject'])\n",
    "# Remove rows with missing values in key columns\n",
    "df = df.dropna(subset=['Explanation', 'Question', 'Answer', 'subject'])\n",
    "\n",
    "# Only keep Physics and Chemistry\n",
    "df = df[df['subject'].str.lower().isin(['physics', 'chemistry'])]\n",
    "\n",
    "print(f\"Total rows after filtering: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058eb458",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-small\"  # Better QA performance than base t5-small\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20cdad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 64\n",
    "\n",
    "input_encodings = tokenizer(\n",
    "    inputs, max_length=max_input_length, truncation=True, padding='max_length', return_tensors='tf'\n",
    ")\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    target_encodings = tokenizer(\n",
    "        targets, max_length=max_target_length, truncation=True, padding='max_length', return_tensors='tf'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask']\n",
    "    },\n",
    "    target_encodings['input_ids']\n",
    ")).shuffle(200).batch(4)  # Batch size 4; adjust as you like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "class CustomProgressCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"\\nStart of epoch {epoch + 1}\")\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if batch % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"  Completed batch {batch + 1}: loss = {logs['loss']:.4f}\")\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"End of epoch {epoch + 1}: loss = {logs['loss']:.4f}\")\n",
    "\n",
    "model.compile(optimizer=optimizer)\n",
    "model.fit(\n",
    "    dataset,\n",
    "    epochs=8,              # More epochs for small data\n",
    "    verbose=0,\n",
    "    callbacks=[CustomProgressCallback()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a560eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('my_qa_model_tf')\n",
    "tokenizer.save_pretrained('my_qa_model_tf')\n",
    "\n",
    "print(\"Training complete! Model and tokenizer saved in 'my_qa_model_tf/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1247d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "from transformers import TFT5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('my_qa_model_tf')\n",
    "model = TFT5ForConditionalGeneration.from_pretrained('my_qa_model_tf')\n",
    "\n",
    "def answer_qa(explanation, question):\n",
    "    input_text = f\"explanation: {explanation} question: {question}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors='tf').input_ids\n",
    "    outputs = model.generate(input_ids)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example:\n",
    "print(answer_qa(\"static electricity.\", \"What is the phenomenon that causes a spark when touching a doorknob?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFT5ForConditionalGeneration, T5Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "class QASystem:\n",
    "    def __init__(self, model_path='my_qa_model_tf'):\n",
    "        try:\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "            self.model = TFT5ForConditionalGeneration.from_pretrained(model_path)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading model: {str(e)}\")\n",
    "\n",
    "    def answer_qa(self, explanation, question, max_length=128, num_beams=4):\n",
    "        try:\n",
    "            # Input validation\n",
    "            if not explanation or not question:\n",
    "                raise ValueError(\"Both explanation and question must be provided\")\n",
    "\n",
    "            # Prepare input text\n",
    "            input_text = f\"explanation: {explanation} question: {question}\"\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                return_tensors='tf',\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "\n",
    "            # Generate answer\n",
    "            outputs = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            # Decode and clean answer\n",
    "            answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return answer.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error generating answer: {str(e)}\"\n",
    "\n",
    "# Create QA system instance\n",
    "qa_system = QASystem()\n",
    "\n",
    "# Example usage with better context\n",
    "explanation = \"\"\"\n",
    "Static electricity is a phenomenon that occurs when electric charges accumulate on surfaces.\n",
    "When you walk across a carpet, your body builds up electrons through friction.\n",
    "When touching a metal doorknob, these excess charges suddenly transfer to the metal,\n",
    "causing a spark and sometimes a small shock. This is known as electrostatic discharge.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What is the phenomenon that causes a spark when touching a doorknob?\",\n",
    "    \"How does static electricity build up?\",\n",
    "    \"What happens when you touch the doorknob?\"\n",
    "]\n",
    "\n",
    "# Test multiple questions\n",
    "for question in questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: {qa_system.answer_qa(explanation, question)}\")\n",
    "\n",
    "# Example of error handling\n",
    "try:\n",
    "    print(\"\\nTesting with empty input:\")\n",
    "    print(qa_system.answer_qa(\"\", \"\"))\n",
    "except ValueError as e:\n",
    "    print(f\"Caught error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b5f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
